{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prebuilt Datasets and Transforms</h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Objective</h2><ul><li> How to use MNIST prebuilt dataset in pytorch.</li></ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, you will use a prebuilt dataset and then use some prebuilt dataset transforms.</p>\n",
    "<ul>\n",
    "    <li><a href=\"https://#Prebuilt_Dataset\">Prebuilt Datasets</a></li>\n",
    "    <li><a href=\"https://#Torchvision\">Torchvision Transforms</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>10 min</strong></p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparation</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision==0.9.1\n",
      "  Downloading torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch==1.8.1\n",
      "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.1/804.1 MB\u001b[0m \u001b[31m508.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from torchvision==0.9.1) (1.21.6)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from torchvision==0.9.1) (8.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from torch==1.8.1) (4.3.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.5.0\n",
      "    Uninstalling torch-1.5.0:\n",
      "      Successfully uninstalled torch-1.5.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.2.1\n",
      "    Uninstalling torchvision-0.2.1:\n",
      "      Successfully uninstalled torchvision-0.2.1\n",
      "Successfully installed torch-1.8.1 torchvision-0.9.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f647417a130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the libraries will be used for this lab.\n",
    "\n",
    "!pip install torchvision==0.9.1 torch==1.8.1 \n",
    "import torch \n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function for displaying images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data by diagram\n",
    "\n",
    "def show_data(data_sample, shape = (28, 28)):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n",
    "    plt.title('y = ' + str(data_sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Prebuilt_Dataset\">Prebuilt Datasets</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will focus on the following libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command below when you do not have torchvision installed\n",
    "# !mamba install -y torchvision\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import a prebuilt dataset. In this case, use MNIST. You'll work with several of these parameters later by placing a transform object in the argument <code>transform</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f26cd9a0854cef896e1ee99357b30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb199775d314c16a902ada297d607c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c1de4d5b414363ba162049aa5426cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09227bce895c4b8cb9def4105cbf6ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Import the prebuilt dataset into variable dataset\n",
    "\n",
    "\n",
    "dataset = dsets.MNIST(\n",
    "    root = './data',  \n",
    "    download = True, \n",
    "    transform = transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of the dataset object contains a tuple. Let us see whether the first element in the dataset is a tuple and what is in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the first element:  <class 'tuple'>\n",
      "The length of the tuple:  2\n",
      "The shape of the first element in the tuple:  torch.Size([1, 28, 28])\n",
      "The type of the first element in the tuple <class 'torch.Tensor'>\n",
      "The second element in the tuple:  5\n",
      "The type of the second element in the tuple:  <class 'int'>\n",
      "As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\n"
     ]
    }
   ],
   "source": [
    "# Examine whether the elements in dataset MNIST are tuples, and what is in the tuple?\n",
    "\n",
    "print(\"Type of the first element: \", type(dataset[0]))\n",
    "print(\"The length of the tuple: \", len(dataset[0]))\n",
    "print(\"The shape of the first element in the tuple: \", dataset[0][0].shape)\n",
    "print(\"The type of the first element in the tuple\", type(dataset[0][0]))\n",
    "print(\"The second element in the tuple: \", dataset[0][1])\n",
    "print(\"The type of the second element in the tuple: \", type(dataset[0][1]))\n",
    "print(\"As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the output, the first element in the tuple is a cuboid tensor. As you can see, there is a dimension with only size 1, so basically, it is a rectangular tensor.<br>\n",
    "The second element in the tuple is a number tensor, which indicate the real number the image shows. As the second element in the tuple is <code>tensor(7)</code>, the image should show a hand-written 7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the first element in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the first element in the dataset\n",
    "\n",
    "show_data(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it is a 7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the second sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the second element in the dataset\n",
    "\n",
    "show_data(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Torchvision\"> Torchvision Transforms  </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply some image transform functions on the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the images in the MNIST dataset can be cropped and converted to a tensor. We can use <code>transform.Compose</code> we learned from the previous lab to combine the two transform functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the first element in the first tuple:  torch.Size([1, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "# Combine two transforms: crop and convert to tensor. Apply the compose to MNIST dataset\n",
    "\n",
    "croptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()])\n",
    "dataset = dsets.MNIST(root = './data', download = True, transform = croptensor_data_transform)\n",
    "print(\"The shape of the first element in the first tuple: \", dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the image is now 20 x 20 instead of 28 x 28.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the first image again. Notice that the black space around the <b>7</b> become less apparent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first element in the dataset\n",
    "\n",
    "show_data(dataset[0],shape = (20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the second element in the dataset\n",
    "\n",
    "show_data(dataset[1],shape = (20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, we horizontally flip the image, and then convert it to a tensor. Use <code>transforms.Compose()</code> to combine these two transform functions. Plot the flipped image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the compose. Apply it on MNIST dataset. Plot the image out.\n",
    "\n",
    "fliptensor_data_transform = transforms.Compose([transforms.RandomHorizontalFlip(p = 1),transforms.ToTensor()])\n",
    "dataset = dsets.MNIST(root = './data', download = True, transform = fliptensor_data_transform)\n",
    "show_data(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Practice</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use the <code>RandomVerticalFlip</code> (vertically flip the image) with horizontally flip and convert to tensor as a compose. Apply the compose on image. Use <code>show_data()</code> to plot the second image (the image as <b>2</b>).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Combine vertical flip, horizontal flip and convert to tensor as a compose. Apply the compose on image. Then plot the image\n",
    "\n",
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "my_data_transform = transforms.Compose([transforms.RandomVerticalFlip(p = 1), transforms.RandomHorizontalFlip(p = 1), transforms.ToTensor()])\n",
    "dataset = dsets.MNIST(root = './data', train = False, download = True, transform = my_data_transform)\n",
    "show_data(dataset[1])\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2022-01-01&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2022-01-01\">Michelle Carey</a>, <a href=\"https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2022-01-01\">Mavis Zhou</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
