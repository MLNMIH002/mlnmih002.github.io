{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Loading and Cleaning the Dataset\nLet's start by importing the pandas, os, ibm_boto3 libraries."}, {"metadata": {}, "cell_type": "code", "source": "\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_3ac1f00a48a141178e622b5fd5d48dd4 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='gpFEgEygpV5FTETUMEGMjUrXeo1CJv7RHwE2gmrbUzIn',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n\nbody = client_3ac1f00a48a141178e622b5fd5d48dd4.get_object(Bucket='buildaregressionmodelinkeras-donotdelete-pr-gxlp3ywxwmjhow',Key='concrete_data.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nconcrete_data  = pd.read_csv(body)\n# looking at the first 5 rows of the data\nconcrete_data.head()", "execution_count": 1, "outputs": [{"output_type": "execute_result", "execution_count": 1, "data": {"text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  Strength  \n0            1040.0           676.0   28     79.99  \n1            1055.0           676.0   28     61.89  \n2             932.0           594.0  270     40.27  \n3             932.0           594.0  365     41.05  \n4             978.4           825.5  360     44.30  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.27</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.05</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Let's check how many data points we have.\n"}, {"metadata": {}, "cell_type": "code", "source": "concrete_data.shape", "execution_count": 2, "outputs": [{"output_type": "execute_result", "execution_count": 2, "data": {"text/plain": "(1030, 9)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Let's check the dataset for any missing values."}, {"metadata": {}, "cell_type": "code", "source": "concrete_data.describe()", "execution_count": 3, "outputs": [{"output_type": "execute_result", "execution_count": 3, "data": {"text/plain": "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\ncount  1030.000000         1030.000000  1030.000000  1030.000000   \nmean    281.167864           73.895825    54.188350   181.567282   \nstd     104.506364           86.279342    63.997004    21.354219   \nmin     102.000000            0.000000     0.000000   121.800000   \n25%     192.375000            0.000000     0.000000   164.900000   \n50%     272.900000           22.000000     0.000000   185.000000   \n75%     350.000000          142.950000   118.300000   192.000000   \nmax     540.000000          359.400000   200.100000   247.000000   \n\n       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\ncount       1030.000000       1030.000000     1030.000000  1030.000000   \nmean           6.204660        972.918932      773.580485    45.662136   \nstd            5.973841         77.753954       80.175980    63.169912   \nmin            0.000000        801.000000      594.000000     1.000000   \n25%            0.000000        932.000000      730.950000     7.000000   \n50%            6.400000        968.000000      779.500000    28.000000   \n75%           10.200000       1029.400000      824.000000    56.000000   \nmax           32.200000       1145.000000      992.600000   365.000000   \n\n          Strength  \ncount  1030.000000  \nmean     35.817961  \nstd      16.705742  \nmin       2.330000  \n25%      23.710000  \n50%      34.445000  \n75%      46.135000  \nmax      82.600000  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n      <th>Strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n      <td>1030.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>281.167864</td>\n      <td>73.895825</td>\n      <td>54.188350</td>\n      <td>181.567282</td>\n      <td>6.204660</td>\n      <td>972.918932</td>\n      <td>773.580485</td>\n      <td>45.662136</td>\n      <td>35.817961</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>104.506364</td>\n      <td>86.279342</td>\n      <td>63.997004</td>\n      <td>21.354219</td>\n      <td>5.973841</td>\n      <td>77.753954</td>\n      <td>80.175980</td>\n      <td>63.169912</td>\n      <td>16.705742</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>102.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>121.800000</td>\n      <td>0.000000</td>\n      <td>801.000000</td>\n      <td>594.000000</td>\n      <td>1.000000</td>\n      <td>2.330000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>192.375000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>164.900000</td>\n      <td>0.000000</td>\n      <td>932.000000</td>\n      <td>730.950000</td>\n      <td>7.000000</td>\n      <td>23.710000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>272.900000</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n      <td>185.000000</td>\n      <td>6.400000</td>\n      <td>968.000000</td>\n      <td>779.500000</td>\n      <td>28.000000</td>\n      <td>34.445000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>350.000000</td>\n      <td>142.950000</td>\n      <td>118.300000</td>\n      <td>192.000000</td>\n      <td>10.200000</td>\n      <td>1029.400000</td>\n      <td>824.000000</td>\n      <td>56.000000</td>\n      <td>46.135000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>540.000000</td>\n      <td>359.400000</td>\n      <td>200.100000</td>\n      <td>247.000000</td>\n      <td>32.200000</td>\n      <td>1145.000000</td>\n      <td>992.600000</td>\n      <td>365.000000</td>\n      <td>82.600000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "concrete_data.isnull().sum()", "execution_count": 4, "outputs": [{"output_type": "execute_result", "execution_count": 4, "data": {"text/plain": "Cement                0\nBlast Furnace Slag    0\nFly Ash               0\nWater                 0\nSuperplasticizer      0\nCoarse Aggregate      0\nFine Aggregate        0\nAge                   0\nStrength              0\ndtype: int64"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "The data looks very clean and is ready to be used to build our model."}, {"metadata": {}, "cell_type": "markdown", "source": "## Split data into predictors and target\nThe target variable in this problem is the concrete sample strength. Therefore, our predictors will be all the other columns."}, {"metadata": {}, "cell_type": "code", "source": "concrete_data_columns = concrete_data.columns\n\npredictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\ntarget = concrete_data['Strength'] # Strength column", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let's do a quick sanity check of the predictors and the target dataframes."}, {"metadata": {}, "cell_type": "code", "source": "predictors.head()", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age  \n0            1040.0           676.0   28  \n1            1055.0           676.0   28  \n2             932.0           594.0  270  \n3             932.0           594.0  365  \n4             978.4           825.5  360  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "target.head()", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "0    79.99\n1    61.89\n2    40.27\n3    41.05\n4    44.30\nName: Strength, dtype: float64"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Train Test Split\nTrain/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive. After which, you train with the training set and test with the testing set.\n\nThis will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that has been used to train the model. It is more realistic for the real world problems."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split( predictors, target, test_size= 0.3, random_state=42)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "Train set: (721, 8) (721,)\nTest set: (309, 8) (309,)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Import the rest of the packages from the Keras library that we will need to build our regressoin model"}, {"metadata": {}, "cell_type": "code", "source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let's save the number of predictors to n_cols since we will need this number when building our network."}, {"metadata": {}, "cell_type": "code", "source": "n_cols = X_train.shape[1] # number of predictors\nn_cols", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "8"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# A. Build a baseline model"}, {"metadata": {}, "cell_type": "markdown", "source": "## Building a Neural Network"}, {"metadata": {}, "cell_type": "code", "source": "# define regression model\ndef regression_model():\n    \n    # create model\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Train and Test the Network\nLet us build the model, fit the model in training data and evaluated it using test data"}, {"metadata": {}, "cell_type": "code", "source": "# building the model\nmodel = regression_model()", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# fitting the model\nmodel.fit(X_train, y_train, epochs=50, verbose=2)", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "Epoch 1/50\n23/23 - 0s - loss: 103.7073 - 38ms/epoch - 2ms/step\nEpoch 2/50\n23/23 - 0s - loss: 100.6752 - 34ms/epoch - 1ms/step\nEpoch 3/50\n23/23 - 0s - loss: 99.8403 - 34ms/epoch - 1ms/step\nEpoch 4/50\n23/23 - 0s - loss: 99.0865 - 33ms/epoch - 1ms/step\nEpoch 5/50\n23/23 - 0s - loss: 97.8687 - 34ms/epoch - 1ms/step\nEpoch 6/50\n23/23 - 0s - loss: 96.9783 - 34ms/epoch - 1ms/step\nEpoch 7/50\n23/23 - 0s - loss: 96.2151 - 35ms/epoch - 2ms/step\nEpoch 8/50\n23/23 - 0s - loss: 96.1324 - 38ms/epoch - 2ms/step\nEpoch 9/50\n23/23 - 0s - loss: 95.3707 - 46ms/epoch - 2ms/step\nEpoch 10/50\n23/23 - 0s - loss: 94.3086 - 36ms/epoch - 2ms/step\nEpoch 11/50\n23/23 - 0s - loss: 92.9856 - 41ms/epoch - 2ms/step\nEpoch 12/50\n23/23 - 0s - loss: 92.9381 - 35ms/epoch - 2ms/step\nEpoch 13/50\n23/23 - 0s - loss: 91.4945 - 35ms/epoch - 2ms/step\nEpoch 14/50\n23/23 - 0s - loss: 91.0185 - 34ms/epoch - 1ms/step\nEpoch 15/50\n23/23 - 0s - loss: 90.5399 - 35ms/epoch - 2ms/step\nEpoch 16/50\n23/23 - 0s - loss: 90.3754 - 34ms/epoch - 1ms/step\nEpoch 17/50\n23/23 - 0s - loss: 89.1337 - 35ms/epoch - 2ms/step\nEpoch 18/50\n23/23 - 0s - loss: 89.0627 - 34ms/epoch - 1ms/step\nEpoch 19/50\n23/23 - 0s - loss: 88.1596 - 36ms/epoch - 2ms/step\nEpoch 20/50\n23/23 - 0s - loss: 88.1506 - 34ms/epoch - 1ms/step\nEpoch 21/50\n23/23 - 0s - loss: 87.3506 - 34ms/epoch - 1ms/step\nEpoch 22/50\n23/23 - 0s - loss: 87.0812 - 34ms/epoch - 1ms/step\nEpoch 23/50\n23/23 - 0s - loss: 87.3848 - 35ms/epoch - 2ms/step\nEpoch 24/50\n23/23 - 0s - loss: 86.8196 - 35ms/epoch - 2ms/step\nEpoch 25/50\n23/23 - 0s - loss: 86.4530 - 35ms/epoch - 2ms/step\nEpoch 26/50\n23/23 - 0s - loss: 85.8013 - 35ms/epoch - 2ms/step\nEpoch 27/50\n23/23 - 0s - loss: 85.5751 - 35ms/epoch - 2ms/step\nEpoch 28/50\n23/23 - 0s - loss: 85.4712 - 34ms/epoch - 1ms/step\nEpoch 29/50\n23/23 - 0s - loss: 84.4285 - 34ms/epoch - 1ms/step\nEpoch 30/50\n23/23 - 0s - loss: 82.6770 - 34ms/epoch - 1ms/step\nEpoch 31/50\n23/23 - 0s - loss: 82.2728 - 34ms/epoch - 1ms/step\nEpoch 32/50\n23/23 - 0s - loss: 83.1158 - 35ms/epoch - 2ms/step\nEpoch 33/50\n23/23 - 0s - loss: 82.6785 - 33ms/epoch - 1ms/step\nEpoch 34/50\n23/23 - 0s - loss: 80.5365 - 35ms/epoch - 2ms/step\nEpoch 35/50\n23/23 - 0s - loss: 80.3082 - 32ms/epoch - 1ms/step\nEpoch 36/50\n23/23 - 0s - loss: 80.3729 - 34ms/epoch - 1ms/step\nEpoch 37/50\n23/23 - 0s - loss: 79.7877 - 34ms/epoch - 1ms/step\nEpoch 38/50\n23/23 - 0s - loss: 78.6608 - 35ms/epoch - 2ms/step\nEpoch 39/50\n23/23 - 0s - loss: 78.3785 - 35ms/epoch - 2ms/step\nEpoch 40/50\n23/23 - 0s - loss: 78.1565 - 33ms/epoch - 1ms/step\nEpoch 41/50\n23/23 - 0s - loss: 78.3192 - 33ms/epoch - 1ms/step\nEpoch 42/50\n23/23 - 0s - loss: 77.6534 - 34ms/epoch - 1ms/step\nEpoch 43/50\n23/23 - 0s - loss: 79.0043 - 34ms/epoch - 1ms/step\nEpoch 44/50\n23/23 - 0s - loss: 79.4554 - 33ms/epoch - 1ms/step\nEpoch 45/50\n23/23 - 0s - loss: 76.3310 - 33ms/epoch - 1ms/step\nEpoch 46/50\n23/23 - 0s - loss: 76.8104 - 33ms/epoch - 1ms/step\nEpoch 47/50\n23/23 - 0s - loss: 75.9055 - 36ms/epoch - 2ms/step\nEpoch 48/50\n23/23 - 0s - loss: 76.7439 - 34ms/epoch - 1ms/step\nEpoch 49/50\n23/23 - 0s - loss: 76.1593 - 34ms/epoch - 1ms/step\nEpoch 50/50\n23/23 - 0s - loss: 76.0974 - 34ms/epoch - 1ms/step\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "<keras.callbacks.History at 0x7f5ca47d54c0>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# evaluating the model\nscores = model.evaluate(X_test, y_test)\nscores", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "10/10 [==============================] - 0s 2ms/step - loss: 82.9886\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "82.98860168457031"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Computing the mean squared error between the predicted concrete strength and the actual concrete strength."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import mean_squared_error\n# Predicting the output using the the test data\nyhat = model.predict(X_test)\n#compute the mean squared error between the predicted concrete strength and the actual concrete strength.\nmean_squared_error(yhat, y_test)", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "82.98860134420103"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Creating a list of 50 mean squared errors and Reporting the mean and the standard deviation of the mean squared errors."}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nMSEs = []\n# creating a for lopp in range 0 to 50\nfor i in range(0, 50):\n    X_train, X_test, y_train, y_test = train_test_split( predictors, target, test_size=0.3, random_state=42)\n    model.fit(X_train, y_train, epochs=50, verbose=0)\n    yhat = model.predict(X_test)\n    MSE = mean_squared_error(yhat, y_test)\n    print(\"Mean_Squared_Error \"+str(i)+\": \"+str(MSE))\n    MSEs.append(MSE)\n    \nMSEs = np.array(MSEs)\nmean = np.mean(MSEs)\nSTDV = np.std(MSEs)\n\nprint('\\n')\nprint(\"Report the mean and the standard deviation of the mean squared errors is indicated below as having values\")\nprint(\"Mean: \",(mean))\nprint(\"Standard Deviation: \",(STDV))", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "Mean_Squared_Error 0: 72.2748989185133\nMean_Squared_Error 1: 68.16269143806839\nMean_Squared_Error 2: 56.75376115999655\nMean_Squared_Error 3: 52.120970035382506\nMean_Squared_Error 4: 51.58000306819858\nMean_Squared_Error 5: 51.25409004620329\nMean_Squared_Error 6: 61.51221593505675\nMean_Squared_Error 7: 48.8183693159458\nMean_Squared_Error 8: 50.54280963245217\nMean_Squared_Error 9: 49.15823451903464\nMean_Squared_Error 10: 54.935755700587016\nMean_Squared_Error 11: 48.557074823293945\nMean_Squared_Error 12: 50.083872899183234\nMean_Squared_Error 13: 50.70672002540613\nMean_Squared_Error 14: 62.854400846010435\nMean_Squared_Error 15: 51.655639603281514\nMean_Squared_Error 16: 49.391280048187824\nMean_Squared_Error 17: 62.072657609761656\nMean_Squared_Error 18: 48.58279990353341\nMean_Squared_Error 19: 48.51324553438091\nMean_Squared_Error 20: 52.28405553996671\nMean_Squared_Error 21: 48.62868500305548\nMean_Squared_Error 22: 48.55015636942144\nMean_Squared_Error 23: 51.98977136755898\nMean_Squared_Error 24: 48.84452434257303\nMean_Squared_Error 25: 51.698250610790076\nMean_Squared_Error 26: 52.445229778123625\nMean_Squared_Error 27: 51.89528892232923\nMean_Squared_Error 28: 51.96116364747751\nMean_Squared_Error 29: 54.68159625842069\nMean_Squared_Error 30: 58.80969215396614\nMean_Squared_Error 31: 54.77038992021079\nMean_Squared_Error 32: 56.372199938445725\nMean_Squared_Error 33: 54.11342460488321\nMean_Squared_Error 34: 49.33305543812305\nMean_Squared_Error 35: 48.37847347773361\nMean_Squared_Error 36: 60.92631377841774\nMean_Squared_Error 37: 48.137490166311736\nMean_Squared_Error 38: 49.82369350535042\nMean_Squared_Error 39: 55.26248659540532\nMean_Squared_Error 40: 48.55591513926677\nMean_Squared_Error 41: 50.0676136131468\nMean_Squared_Error 42: 49.00034374552422\nMean_Squared_Error 43: 52.97135180031845\nMean_Squared_Error 44: 51.221726681710244\nMean_Squared_Error 45: 53.375118032098484\nMean_Squared_Error 46: 48.45689344084468\nMean_Squared_Error 47: 57.266196078585544\nMean_Squared_Error 48: 48.47562501861834\nMean_Squared_Error 49: 55.22457476428746\n\n\nReport the mean and the standard deviation of the mean squared errors is indicated below as having values\nMean:  53.061055815908865\nStandard Deviation:  5.2264455372428325\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "# B. Normalize the data\n\nNormalizing the data by substracting the mean and dividing by the standard deviation."}, {"metadata": {}, "cell_type": "code", "source": "#Normalising the data\npredictors_norm = (predictors - predictors.mean()) / predictors.std()\npredictors_norm.head()", "execution_count": 31, "outputs": [{"output_type": "execute_result", "execution_count": 31, "data": {"text/plain": "     Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n0  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n1  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n2  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n3  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n4 -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n\n   Coarse Aggregate  Fine Aggregate       Age  \n0          0.862735       -1.217079 -0.279597  \n1          1.055651       -1.217079 -0.279597  \n2         -0.526262       -2.239829  3.551340  \n3         -0.526262       -2.239829  5.055221  \n4          0.070492        0.647569  4.976069  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.476712</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>-0.916319</td>\n      <td>-0.620147</td>\n      <td>0.862735</td>\n      <td>-1.217079</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.476712</td>\n      <td>-0.856472</td>\n      <td>-0.846733</td>\n      <td>-0.916319</td>\n      <td>-0.620147</td>\n      <td>1.055651</td>\n      <td>-1.217079</td>\n      <td>-0.279597</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.491187</td>\n      <td>0.795140</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>3.551340</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.491187</td>\n      <td>0.795140</td>\n      <td>-0.846733</td>\n      <td>2.174405</td>\n      <td>-1.038638</td>\n      <td>-0.526262</td>\n      <td>-2.239829</td>\n      <td>5.055221</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.790075</td>\n      <td>0.678079</td>\n      <td>-0.846733</td>\n      <td>0.488555</td>\n      <td>-1.038638</td>\n      <td>0.070492</td>\n      <td>0.647569</td>\n      <td>4.976069</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "#Spliting the data into train and testing data\nX_train2, X_test2, y_train2, y_test2 = train_test_split( predictors_norm, target, test_size= 0.3, random_state=42)\nprint ('Train set:', X_train2.shape,  y_train2.shape)\nprint ('Test set:', X_test2.shape,  y_test2.shape)", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "Train set: (721, 8) (721,)\nTest set: (309, 8) (309,)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# fitting the model\nmodel.fit(X_train2, y_train2, epochs=50, verbose=2)\n# evaluating the model\nscores = model.evaluate(X_test2, y_test2)\nscores", "execution_count": 35, "outputs": [{"output_type": "stream", "text": "Epoch 1/50\n23/23 - 0s - loss: 82.3887 - 47ms/epoch - 2ms/step\nEpoch 2/50\n23/23 - 0s - loss: 82.1157 - 40ms/epoch - 2ms/step\nEpoch 3/50\n23/23 - 0s - loss: 81.9280 - 37ms/epoch - 2ms/step\nEpoch 4/50\n23/23 - 0s - loss: 81.6107 - 35ms/epoch - 2ms/step\nEpoch 5/50\n23/23 - 0s - loss: 81.4684 - 34ms/epoch - 1ms/step\nEpoch 6/50\n23/23 - 0s - loss: 81.1863 - 34ms/epoch - 1ms/step\nEpoch 7/50\n23/23 - 0s - loss: 80.9220 - 35ms/epoch - 2ms/step\nEpoch 8/50\n23/23 - 0s - loss: 80.7255 - 33ms/epoch - 1ms/step\nEpoch 9/50\n23/23 - 0s - loss: 80.5294 - 33ms/epoch - 1ms/step\nEpoch 10/50\n23/23 - 0s - loss: 80.2137 - 34ms/epoch - 1ms/step\nEpoch 11/50\n23/23 - 0s - loss: 79.9950 - 37ms/epoch - 2ms/step\nEpoch 12/50\n23/23 - 0s - loss: 79.7323 - 44ms/epoch - 2ms/step\nEpoch 13/50\n23/23 - 0s - loss: 79.4734 - 35ms/epoch - 2ms/step\nEpoch 14/50\n23/23 - 0s - loss: 79.2804 - 37ms/epoch - 2ms/step\nEpoch 15/50\n23/23 - 0s - loss: 79.0284 - 36ms/epoch - 2ms/step\nEpoch 16/50\n23/23 - 0s - loss: 78.8497 - 33ms/epoch - 1ms/step\nEpoch 17/50\n23/23 - 0s - loss: 78.5824 - 34ms/epoch - 1ms/step\nEpoch 18/50\n23/23 - 0s - loss: 78.3243 - 35ms/epoch - 2ms/step\nEpoch 19/50\n23/23 - 0s - loss: 78.0836 - 33ms/epoch - 1ms/step\nEpoch 20/50\n23/23 - 0s - loss: 77.8527 - 34ms/epoch - 1ms/step\nEpoch 21/50\n23/23 - 0s - loss: 77.7480 - 33ms/epoch - 1ms/step\nEpoch 22/50\n23/23 - 0s - loss: 77.3908 - 33ms/epoch - 1ms/step\nEpoch 23/50\n23/23 - 0s - loss: 77.1475 - 34ms/epoch - 1ms/step\nEpoch 24/50\n23/23 - 0s - loss: 76.9695 - 36ms/epoch - 2ms/step\nEpoch 25/50\n23/23 - 0s - loss: 76.6685 - 36ms/epoch - 2ms/step\nEpoch 26/50\n23/23 - 0s - loss: 76.4574 - 38ms/epoch - 2ms/step\nEpoch 27/50\n23/23 - 0s - loss: 76.1901 - 34ms/epoch - 1ms/step\nEpoch 28/50\n23/23 - 0s - loss: 76.0243 - 36ms/epoch - 2ms/step\nEpoch 29/50\n23/23 - 0s - loss: 75.7630 - 36ms/epoch - 2ms/step\nEpoch 30/50\n23/23 - 0s - loss: 75.5067 - 38ms/epoch - 2ms/step\nEpoch 31/50\n23/23 - 0s - loss: 75.2511 - 35ms/epoch - 2ms/step\nEpoch 32/50\n23/23 - 0s - loss: 75.0550 - 34ms/epoch - 1ms/step\nEpoch 33/50\n23/23 - 0s - loss: 74.8053 - 40ms/epoch - 2ms/step\nEpoch 34/50\n23/23 - 0s - loss: 74.5637 - 37ms/epoch - 2ms/step\nEpoch 35/50\n23/23 - 0s - loss: 74.3642 - 39ms/epoch - 2ms/step\nEpoch 36/50\n23/23 - 0s - loss: 74.0935 - 36ms/epoch - 2ms/step\nEpoch 37/50\n23/23 - 0s - loss: 73.8861 - 41ms/epoch - 2ms/step\nEpoch 38/50\n23/23 - 0s - loss: 73.6178 - 44ms/epoch - 2ms/step\nEpoch 39/50\n23/23 - 0s - loss: 73.3855 - 37ms/epoch - 2ms/step\nEpoch 40/50\n23/23 - 0s - loss: 73.1590 - 38ms/epoch - 2ms/step\nEpoch 41/50\n23/23 - 0s - loss: 72.9062 - 39ms/epoch - 2ms/step\nEpoch 42/50\n23/23 - 0s - loss: 72.7424 - 38ms/epoch - 2ms/step\nEpoch 43/50\n23/23 - 0s - loss: 72.4721 - 34ms/epoch - 1ms/step\nEpoch 44/50\n23/23 - 0s - loss: 72.2597 - 35ms/epoch - 2ms/step\nEpoch 45/50\n23/23 - 0s - loss: 72.0033 - 35ms/epoch - 2ms/step\nEpoch 46/50\n23/23 - 0s - loss: 71.7945 - 36ms/epoch - 2ms/step\nEpoch 47/50\n23/23 - 0s - loss: 71.5415 - 36ms/epoch - 2ms/step\nEpoch 48/50\n23/23 - 0s - loss: 71.2848 - 37ms/epoch - 2ms/step\nEpoch 49/50\n23/23 - 0s - loss: 71.0593 - 35ms/epoch - 2ms/step\nEpoch 50/50\n23/23 - 0s - loss: 70.8338 - 34ms/epoch - 1ms/step\n10/10 [==============================] - 0s 2ms/step - loss: 75.6016\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 35, "data": {"text/plain": "75.6015853881836"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Creating a list of 50 mean squared errors and Reporting the mean and the standard deviation of the mean squared errors for normalised data."}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nMSEs2 = []\n# creating a for lopp in range 0 to 50\nfor i in range(0, 50):\n    X_train2, X_test2, y_train2, y_test2 = train_test_split( predictors_norm, target, test_size=0.3, random_state=42)\n    model.fit(X_train2, y_train2, epochs=50, verbose=0)\n    yhat2 = model.predict(X_test2)\n    MSE2 = mean_squared_error(yhat2, y_test2)\n    print(\"Mean_Squared_Error \"+str(i)+\": \"+str(MSE2))\n    MSEs2.append(MSE2)\n    \nMSEs2 = np.array(MSEs2)\nmean = np.mean(MSEs2)\nSTDV = np.std(MSEs2)\n\nprint('\\n')\nprint(\"Report the mean and the standard deviation of the mean squared errors for normalised data is indicated below as having values\")\nprint(\"Mean: \",(mean))\nprint(\"Standard Deviation: \",(STDV))", "execution_count": 36, "outputs": [{"output_type": "stream", "text": "Mean_Squared_Error 0: 67.05371614320511\nMean_Squared_Error 1: 60.79063684769401\nMean_Squared_Error 2: 56.79632650780439\nMean_Squared_Error 3: 53.41997214977755\nMean_Squared_Error 4: 52.22371748410707\nMean_Squared_Error 5: 51.06807090451896\nMean_Squared_Error 6: 50.22484861207815\nMean_Squared_Error 7: 49.51325010858649\nMean_Squared_Error 8: 48.82389411081277\nMean_Squared_Error 9: 48.64982110506499\nMean_Squared_Error 10: 47.814590539875496\nMean_Squared_Error 11: 47.41173180836115\nMean_Squared_Error 12: 47.03473197012444\nMean_Squared_Error 13: 46.40415468684569\nMean_Squared_Error 14: 46.17503729657847\nMean_Squared_Error 15: 44.81521350712925\nMean_Squared_Error 16: 44.606753666547675\nMean_Squared_Error 17: 43.93784937312732\nMean_Squared_Error 18: 43.75296630914067\nMean_Squared_Error 19: 43.69435522690286\nMean_Squared_Error 20: 43.6090737195253\nMean_Squared_Error 21: 42.99605296452318\nMean_Squared_Error 22: 42.90356756167183\nMean_Squared_Error 23: 42.45730156755461\nMean_Squared_Error 24: 42.37034379770054\nMean_Squared_Error 25: 42.19911000709627\nMean_Squared_Error 26: 42.04182520965408\nMean_Squared_Error 27: 42.285702791574245\nMean_Squared_Error 28: 42.126739192856256\nMean_Squared_Error 29: 41.96458116361638\nMean_Squared_Error 30: 41.65822869774799\nMean_Squared_Error 31: 41.404946609444714\nMean_Squared_Error 32: 41.29189017249966\nMean_Squared_Error 33: 40.74311845495248\nMean_Squared_Error 34: 41.01921872595294\nMean_Squared_Error 35: 40.84629105760853\nMean_Squared_Error 36: 41.014288624525626\nMean_Squared_Error 37: 40.71089893255921\nMean_Squared_Error 38: 40.94422212260093\nMean_Squared_Error 39: 40.58099532577697\nMean_Squared_Error 40: 40.91124799438792\nMean_Squared_Error 41: 40.7923571864232\nMean_Squared_Error 42: 40.343777630258415\nMean_Squared_Error 43: 40.638102375018605\nMean_Squared_Error 44: 40.51367030289048\nMean_Squared_Error 45: 40.4672191218915\nMean_Squared_Error 46: 40.381528007933404\nMean_Squared_Error 47: 40.437826402129545\nMean_Squared_Error 48: 40.52812006647714\nMean_Squared_Error 49: 40.32105226423223\n\n\nReport the mean and the standard deviation of the mean squared errors for normalised data is indicated below as having values\nMean:  44.694298728187334\nStandard Deviation:  5.578219728288489\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### How does the mean of the mean squared errors compare to that from Step A?\n\nThere is an improvement in the mean from 53.0610 to 44.6942"}, {"metadata": {}, "cell_type": "markdown", "source": "# C. Increate the number of epochs\nRepeat Part B but use 100 epochs this time for training.\n\n"}, {"metadata": {}, "cell_type": "code", "source": "# fitting the model \nmodel.fit(X_train2, y_train2, epochs=100, verbose=2)# number of epoachs increased to 100\n# evaluating the model\nscores = model.evaluate(X_test2, y_test2)\nscores", "execution_count": 37, "outputs": [{"output_type": "stream", "text": "Epoch 1/100\n23/23 - 0s - loss: 35.3943 - 33ms/epoch - 1ms/step\nEpoch 2/100\n23/23 - 0s - loss: 35.4037 - 32ms/epoch - 1ms/step\nEpoch 3/100\n23/23 - 0s - loss: 35.3494 - 34ms/epoch - 1ms/step\nEpoch 4/100\n23/23 - 0s - loss: 35.3554 - 32ms/epoch - 1ms/step\nEpoch 5/100\n23/23 - 0s - loss: 35.3661 - 33ms/epoch - 1ms/step\nEpoch 6/100\n23/23 - 0s - loss: 35.4570 - 34ms/epoch - 1ms/step\nEpoch 7/100\n23/23 - 0s - loss: 35.4132 - 32ms/epoch - 1ms/step\nEpoch 8/100\n23/23 - 0s - loss: 35.3200 - 30ms/epoch - 1ms/step\nEpoch 9/100\n23/23 - 0s - loss: 35.3203 - 31ms/epoch - 1ms/step\nEpoch 10/100\n23/23 - 0s - loss: 35.3633 - 32ms/epoch - 1ms/step\nEpoch 11/100\n23/23 - 0s - loss: 35.3555 - 38ms/epoch - 2ms/step\nEpoch 12/100\n23/23 - 0s - loss: 35.4012 - 32ms/epoch - 1ms/step\nEpoch 13/100\n23/23 - 0s - loss: 35.3805 - 32ms/epoch - 1ms/step\nEpoch 14/100\n23/23 - 0s - loss: 35.3341 - 32ms/epoch - 1ms/step\nEpoch 15/100\n23/23 - 0s - loss: 35.3749 - 33ms/epoch - 1ms/step\nEpoch 16/100\n23/23 - 0s - loss: 35.3433 - 33ms/epoch - 1ms/step\nEpoch 17/100\n23/23 - 0s - loss: 35.3823 - 33ms/epoch - 1ms/step\nEpoch 18/100\n23/23 - 0s - loss: 35.3310 - 33ms/epoch - 1ms/step\nEpoch 19/100\n23/23 - 0s - loss: 35.3324 - 33ms/epoch - 1ms/step\nEpoch 20/100\n23/23 - 0s - loss: 35.3512 - 33ms/epoch - 1ms/step\nEpoch 21/100\n23/23 - 0s - loss: 35.3318 - 34ms/epoch - 1ms/step\nEpoch 22/100\n23/23 - 0s - loss: 35.4266 - 36ms/epoch - 2ms/step\nEpoch 23/100\n23/23 - 0s - loss: 35.2849 - 35ms/epoch - 2ms/step\nEpoch 24/100\n23/23 - 0s - loss: 35.3540 - 35ms/epoch - 2ms/step\nEpoch 25/100\n23/23 - 0s - loss: 35.3209 - 35ms/epoch - 2ms/step\nEpoch 26/100\n23/23 - 0s - loss: 35.3862 - 33ms/epoch - 1ms/step\nEpoch 27/100\n23/23 - 0s - loss: 35.3250 - 35ms/epoch - 2ms/step\nEpoch 28/100\n23/23 - 0s - loss: 35.3204 - 32ms/epoch - 1ms/step\nEpoch 29/100\n23/23 - 0s - loss: 35.3596 - 33ms/epoch - 1ms/step\nEpoch 30/100\n23/23 - 0s - loss: 35.3283 - 32ms/epoch - 1ms/step\nEpoch 31/100\n23/23 - 0s - loss: 35.3382 - 33ms/epoch - 1ms/step\nEpoch 32/100\n23/23 - 0s - loss: 35.3615 - 32ms/epoch - 1ms/step\nEpoch 33/100\n23/23 - 0s - loss: 35.3250 - 34ms/epoch - 1ms/step\nEpoch 34/100\n23/23 - 0s - loss: 35.3305 - 33ms/epoch - 1ms/step\nEpoch 35/100\n23/23 - 0s - loss: 35.3510 - 33ms/epoch - 1ms/step\nEpoch 36/100\n23/23 - 0s - loss: 35.3339 - 31ms/epoch - 1ms/step\nEpoch 37/100\n23/23 - 0s - loss: 35.3399 - 33ms/epoch - 1ms/step\nEpoch 38/100\n23/23 - 0s - loss: 35.3171 - 33ms/epoch - 1ms/step\nEpoch 39/100\n23/23 - 0s - loss: 35.2936 - 49ms/epoch - 2ms/step\nEpoch 40/100\n23/23 - 0s - loss: 35.3253 - 33ms/epoch - 1ms/step\nEpoch 41/100\n23/23 - 0s - loss: 35.3746 - 32ms/epoch - 1ms/step\nEpoch 42/100\n23/23 - 0s - loss: 35.3499 - 38ms/epoch - 2ms/step\nEpoch 43/100\n23/23 - 0s - loss: 35.3196 - 34ms/epoch - 1ms/step\nEpoch 44/100\n23/23 - 0s - loss: 35.2784 - 34ms/epoch - 1ms/step\nEpoch 45/100\n23/23 - 0s - loss: 35.3582 - 35ms/epoch - 2ms/step\nEpoch 46/100\n23/23 - 0s - loss: 35.3920 - 32ms/epoch - 1ms/step\nEpoch 47/100\n23/23 - 0s - loss: 35.2896 - 33ms/epoch - 1ms/step\nEpoch 48/100\n23/23 - 0s - loss: 35.3842 - 34ms/epoch - 1ms/step\nEpoch 49/100\n23/23 - 0s - loss: 35.3034 - 33ms/epoch - 1ms/step\nEpoch 50/100\n23/23 - 0s - loss: 35.2832 - 32ms/epoch - 1ms/step\nEpoch 51/100\n23/23 - 0s - loss: 35.3246 - 33ms/epoch - 1ms/step\nEpoch 52/100\n23/23 - 0s - loss: 35.2589 - 32ms/epoch - 1ms/step\nEpoch 53/100\n23/23 - 0s - loss: 35.3026 - 35ms/epoch - 2ms/step\nEpoch 54/100\n23/23 - 0s - loss: 35.3655 - 36ms/epoch - 2ms/step\nEpoch 55/100\n23/23 - 0s - loss: 35.2898 - 34ms/epoch - 1ms/step\nEpoch 56/100\n23/23 - 0s - loss: 35.3561 - 35ms/epoch - 2ms/step\nEpoch 57/100\n23/23 - 0s - loss: 35.2962 - 35ms/epoch - 2ms/step\nEpoch 58/100\n23/23 - 0s - loss: 35.3180 - 35ms/epoch - 2ms/step\nEpoch 59/100\n23/23 - 0s - loss: 35.2650 - 35ms/epoch - 2ms/step\nEpoch 60/100\n23/23 - 0s - loss: 35.3125 - 35ms/epoch - 2ms/step\nEpoch 61/100\n23/23 - 0s - loss: 35.3365 - 35ms/epoch - 2ms/step\nEpoch 62/100\n23/23 - 0s - loss: 35.2974 - 34ms/epoch - 1ms/step\nEpoch 63/100\n23/23 - 0s - loss: 35.2685 - 35ms/epoch - 2ms/step\nEpoch 64/100\n23/23 - 0s - loss: 35.2460 - 35ms/epoch - 2ms/step\nEpoch 65/100\n23/23 - 0s - loss: 35.2454 - 35ms/epoch - 2ms/step\nEpoch 66/100\n23/23 - 0s - loss: 35.3312 - 34ms/epoch - 1ms/step\nEpoch 67/100\n23/23 - 0s - loss: 35.3760 - 32ms/epoch - 1ms/step\nEpoch 68/100\n23/23 - 0s - loss: 35.2764 - 33ms/epoch - 1ms/step\nEpoch 69/100\n23/23 - 0s - loss: 35.2861 - 35ms/epoch - 2ms/step\nEpoch 70/100\n23/23 - 0s - loss: 35.2344 - 33ms/epoch - 1ms/step\nEpoch 71/100\n23/23 - 0s - loss: 35.3404 - 35ms/epoch - 2ms/step\nEpoch 72/100\n23/23 - 0s - loss: 35.3023 - 35ms/epoch - 2ms/step\nEpoch 73/100\n23/23 - 0s - loss: 35.2507 - 32ms/epoch - 1ms/step\nEpoch 74/100\n23/23 - 0s - loss: 35.2647 - 33ms/epoch - 1ms/step\nEpoch 75/100\n23/23 - 0s - loss: 35.2373 - 34ms/epoch - 1ms/step\nEpoch 76/100\n23/23 - 0s - loss: 35.2707 - 34ms/epoch - 1ms/step\nEpoch 77/100\n23/23 - 0s - loss: 35.2765 - 34ms/epoch - 1ms/step\nEpoch 78/100\n23/23 - 0s - loss: 35.2541 - 34ms/epoch - 1ms/step\nEpoch 79/100\n23/23 - 0s - loss: 35.2208 - 34ms/epoch - 1ms/step\nEpoch 80/100\n23/23 - 0s - loss: 35.2628 - 35ms/epoch - 2ms/step\nEpoch 81/100\n23/23 - 0s - loss: 35.2176 - 36ms/epoch - 2ms/step\nEpoch 82/100\n23/23 - 0s - loss: 35.3120 - 41ms/epoch - 2ms/step\nEpoch 83/100\n23/23 - 0s - loss: 35.2718 - 35ms/epoch - 2ms/step\nEpoch 84/100\n23/23 - 0s - loss: 35.2763 - 36ms/epoch - 2ms/step\nEpoch 85/100\n23/23 - 0s - loss: 35.2204 - 36ms/epoch - 2ms/step\nEpoch 86/100\n23/23 - 0s - loss: 35.2744 - 35ms/epoch - 2ms/step\nEpoch 87/100\n23/23 - 0s - loss: 35.2905 - 33ms/epoch - 1ms/step\nEpoch 88/100\n23/23 - 0s - loss: 35.2395 - 34ms/epoch - 1ms/step\nEpoch 89/100\n23/23 - 0s - loss: 35.2708 - 31ms/epoch - 1ms/step\nEpoch 90/100\n23/23 - 0s - loss: 35.1960 - 31ms/epoch - 1ms/step\nEpoch 91/100\n23/23 - 0s - loss: 35.2189 - 31ms/epoch - 1ms/step\nEpoch 92/100\n23/23 - 0s - loss: 35.2448 - 33ms/epoch - 1ms/step\nEpoch 93/100\n23/23 - 0s - loss: 35.2363 - 34ms/epoch - 1ms/step\nEpoch 94/100\n23/23 - 0s - loss: 35.2776 - 35ms/epoch - 2ms/step\nEpoch 95/100\n23/23 - 0s - loss: 35.2609 - 32ms/epoch - 1ms/step\nEpoch 96/100\n23/23 - 0s - loss: 35.2186 - 32ms/epoch - 1ms/step\nEpoch 97/100\n23/23 - 0s - loss: 35.2491 - 34ms/epoch - 1ms/step\nEpoch 98/100\n23/23 - 0s - loss: 35.2449 - 33ms/epoch - 1ms/step\nEpoch 99/100\n23/23 - 0s - loss: 35.2114 - 32ms/epoch - 1ms/step\nEpoch 100/100\n23/23 - 0s - loss: 35.2625 - 34ms/epoch - 1ms/step\n10/10 [==============================] - 0s 2ms/step - loss: 40.1732\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 37, "data": {"text/plain": "40.17315673828125"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# Predicting the output using the the test data\nyhat2 = model.predict(X_test2)\n#compute the mean squared error between the predicted concrete strength and the actual concrete strength.\nmean_squared_error(yhat2, y_test2)", "execution_count": 38, "outputs": [{"output_type": "execute_result", "execution_count": 38, "data": {"text/plain": "40.173160616188795"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nMSEs2 = []\n# creating a for lopp in range 0 to 50\nfor i in range(0, 50):\n    X_train2, X_test2, y_train2, y_test2 = train_test_split( predictors_norm, target, test_size=0.3, random_state=42)\n    model.fit(X_train2, y_train2, epochs=100, verbose=0) #number of epoachs increased to 100\n    yhat2 = model.predict(X_test2)\n    MSE2 = mean_squared_error(yhat2, y_test2)\n    #print(\"Mean_Squared_Error \"+str(i)+\": \"+str(MSE2))\n    MSEs2.append(MSE2)\n    \nMSEs2 = np.array(MSEs2)\nmean = np.mean(MSEs2)\nSTDV = np.std(MSEs2)\n\nprint('\\n')\nprint(\"Report the mean and the standard deviation of the mean squared errors for normalised data is indicated below as having values\")\nprint(\"Mean: \",(mean))\nprint(\"Standard Deviation: \",(STDV))", "execution_count": 39, "outputs": [{"output_type": "stream", "text": "\n\nReport the mean and the standard deviation of the mean squared errors for normalised data is indicated below as having values\nMean:  38.56783607597417\nStandard Deviation:  0.5922653543475711\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### How does the mean of the mean squared errors compare to that from Step B?\n\nThere is an improvement in the mean from 44.6942 to 38.5678"}, {"metadata": {}, "cell_type": "markdown", "source": "# D. Increase the number of hidden layers"}, {"metadata": {}, "cell_type": "markdown", "source": "Repeating part B but use a neural network with the following instead: - Three hidden layers, each of 10 nodes and ReLU activation function."}, {"metadata": {}, "cell_type": "markdown", "source": "### Building a Neural Network"}, {"metadata": {}, "cell_type": "code", "source": "# define regression model\ndef regression_model():\n    \n    # create model\n    model = Sequential()\n    \n    #Three hidden layers, each of 10 nodes and ReLU activation function\n    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1))\n    \n    # compile model\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model", "execution_count": 41, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# building the model\nmodel = regression_model()", "execution_count": 42, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# fitting the model\nmodel.fit(X_train2, y_train2, epochs=50, verbose=2)\n# evaluating the model\nscores = model.evaluate(X_test2, y_test2)\nscores", "execution_count": 46, "outputs": [{"output_type": "stream", "text": "Epoch 1/50\n23/23 - 0s - loss: 18.0073 - 51ms/epoch - 2ms/step\nEpoch 2/50\n23/23 - 0s - loss: 17.8428 - 39ms/epoch - 2ms/step\nEpoch 3/50\n23/23 - 0s - loss: 18.0821 - 42ms/epoch - 2ms/step\nEpoch 4/50\n23/23 - 0s - loss: 18.7195 - 41ms/epoch - 2ms/step\nEpoch 5/50\n23/23 - 0s - loss: 17.8871 - 44ms/epoch - 2ms/step\nEpoch 6/50\n23/23 - 0s - loss: 17.6206 - 45ms/epoch - 2ms/step\nEpoch 7/50\n23/23 - 0s - loss: 17.6418 - 43ms/epoch - 2ms/step\nEpoch 8/50\n23/23 - 0s - loss: 17.7967 - 42ms/epoch - 2ms/step\nEpoch 9/50\n23/23 - 0s - loss: 17.7924 - 44ms/epoch - 2ms/step\nEpoch 10/50\n23/23 - 0s - loss: 18.1402 - 42ms/epoch - 2ms/step\nEpoch 11/50\n23/23 - 0s - loss: 17.8981 - 41ms/epoch - 2ms/step\nEpoch 12/50\n23/23 - 0s - loss: 17.6813 - 39ms/epoch - 2ms/step\nEpoch 13/50\n23/23 - 0s - loss: 17.7302 - 41ms/epoch - 2ms/step\nEpoch 14/50\n23/23 - 0s - loss: 17.7065 - 41ms/epoch - 2ms/step\nEpoch 15/50\n23/23 - 0s - loss: 18.1555 - 43ms/epoch - 2ms/step\nEpoch 16/50\n23/23 - 0s - loss: 17.8765 - 43ms/epoch - 2ms/step\nEpoch 17/50\n23/23 - 0s - loss: 17.7930 - 41ms/epoch - 2ms/step\nEpoch 18/50\n23/23 - 0s - loss: 18.2026 - 40ms/epoch - 2ms/step\nEpoch 19/50\n23/23 - 0s - loss: 18.3001 - 41ms/epoch - 2ms/step\nEpoch 20/50\n23/23 - 0s - loss: 18.0167 - 44ms/epoch - 2ms/step\nEpoch 21/50\n23/23 - 0s - loss: 17.7719 - 41ms/epoch - 2ms/step\nEpoch 22/50\n23/23 - 0s - loss: 17.7206 - 42ms/epoch - 2ms/step\nEpoch 23/50\n23/23 - 0s - loss: 17.7749 - 42ms/epoch - 2ms/step\nEpoch 24/50\n23/23 - 0s - loss: 18.1482 - 42ms/epoch - 2ms/step\nEpoch 25/50\n23/23 - 0s - loss: 18.2263 - 44ms/epoch - 2ms/step\nEpoch 26/50\n23/23 - 0s - loss: 18.5927 - 43ms/epoch - 2ms/step\nEpoch 27/50\n23/23 - 0s - loss: 17.8091 - 41ms/epoch - 2ms/step\nEpoch 28/50\n23/23 - 0s - loss: 18.0846 - 40ms/epoch - 2ms/step\nEpoch 29/50\n23/23 - 0s - loss: 17.8966 - 40ms/epoch - 2ms/step\nEpoch 30/50\n23/23 - 0s - loss: 17.6835 - 38ms/epoch - 2ms/step\nEpoch 31/50\n23/23 - 0s - loss: 18.0516 - 38ms/epoch - 2ms/step\nEpoch 32/50\n23/23 - 0s - loss: 18.2766 - 38ms/epoch - 2ms/step\nEpoch 33/50\n23/23 - 0s - loss: 17.8295 - 40ms/epoch - 2ms/step\nEpoch 34/50\n23/23 - 0s - loss: 17.4853 - 38ms/epoch - 2ms/step\nEpoch 35/50\n23/23 - 0s - loss: 17.9927 - 38ms/epoch - 2ms/step\nEpoch 36/50\n23/23 - 0s - loss: 17.7845 - 39ms/epoch - 2ms/step\nEpoch 37/50\n23/23 - 0s - loss: 17.7789 - 39ms/epoch - 2ms/step\nEpoch 38/50\n23/23 - 0s - loss: 17.5846 - 40ms/epoch - 2ms/step\nEpoch 39/50\n23/23 - 0s - loss: 17.7354 - 41ms/epoch - 2ms/step\nEpoch 40/50\n23/23 - 0s - loss: 18.1982 - 39ms/epoch - 2ms/step\nEpoch 41/50\n23/23 - 0s - loss: 18.3843 - 39ms/epoch - 2ms/step\nEpoch 42/50\n23/23 - 0s - loss: 17.6601 - 39ms/epoch - 2ms/step\nEpoch 43/50\n23/23 - 0s - loss: 17.7517 - 39ms/epoch - 2ms/step\nEpoch 44/50\n23/23 - 0s - loss: 18.3027 - 40ms/epoch - 2ms/step\nEpoch 45/50\n23/23 - 0s - loss: 17.8065 - 39ms/epoch - 2ms/step\nEpoch 46/50\n23/23 - 0s - loss: 17.8120 - 43ms/epoch - 2ms/step\nEpoch 47/50\n23/23 - 0s - loss: 18.6723 - 41ms/epoch - 2ms/step\nEpoch 48/50\n23/23 - 0s - loss: 17.6047 - 40ms/epoch - 2ms/step\nEpoch 49/50\n23/23 - 0s - loss: 18.1860 - 41ms/epoch - 2ms/step\nEpoch 50/50\n23/23 - 0s - loss: 17.9701 - 40ms/epoch - 2ms/step\n10/10 [==============================] - 0s 2ms/step - loss: 34.8340\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 46, "data": {"text/plain": "34.8339729309082"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# Predicting the output using the the test data\nyhat2 = model.predict(X_test2)\n#compute the mean squared error between the predicted concrete strength and the actual concrete strength.\nmean_squared_error(yhat2, y_test2)", "execution_count": 47, "outputs": [{"output_type": "execute_result", "execution_count": 47, "data": {"text/plain": "34.833973007126076"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nMSEs2 = []\n# creating a for lopp in range 0 to 50\nfor i in range(0, 50):\n    X_train2, X_test2, y_train2, y_test2 = train_test_split( predictors_norm, target, test_size=0.3, random_state=42)\n    model.fit(X_train2, y_train2, epochs=50, verbose=0)\n    yhat2 = model.predict(X_test2)\n    MSE2 = mean_squared_error(yhat2, y_test2)\n    #print(\"Mean_Squared_Error \"+str(i)+\": \"+str(MSE2))\n    MSEs2.append(MSE2)\n    \nMSEs2 = np.array(MSEs2)\nmean = np.mean(MSEs2)\nSTDV = np.std(MSEs2)\n\nprint('\\n')\nprint(\"Report the mean and the standard deviation of the mean squared errors for normalised data is indicated below as having values\")\nprint(\"Mean: \",(mean))\nprint(\"Standard Deviation: \",(STDV))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### How does the mean of the mean squared errors compare to that from Step B?\nThere is an improvement in the mean from 44.6942 to 37.3903"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.7", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}